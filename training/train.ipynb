{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a5e7089db861d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# nlp imports\n",
    "import torch\n",
    "import datasets\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from trl import DPOConfig, DPOTrainer, SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from trl.core import PPODecorators\n",
    "\n",
    "from utils import format_function\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def read_config(config_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_train_test(**kwargs):\n",
    "    train_dataset = pd.read_json(f\"../{DATA_FOLDER}/{kwargs['path']}/{kwargs['train_data']}\", lines=True)\n",
    "    test_dataset = pd.read_json(f\"../{DATA_FOLDER}/{kwargs['path']}/{kwargs['test_data']}\", lines=True)\n",
    "\n",
    "    max_data_points = kwargs[\"max_data_points\"]\n",
    "    if max_data_points != \"all\":\n",
    "        max_data_points = int(max_data_points)\n",
    "\n",
    "        if train_dataset.shape[0] > max_data_points:\n",
    "            train_dataset = train_dataset.sample(n=int(max_data_points), random_state=RANDOM_STATE)\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_pandas(train_dataset)\n",
    "    test_dataset = datasets.Dataset.from_pandas(test_dataset)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def train_DPO(model, tokenizer, num_epochs, output_folder, dpo_config, peft_config, **kwargs):\n",
    "    # loading the datasets\n",
    "    train_dataset, test_dataset = load_train_test(**kwargs)\n",
    "\n",
    "    dataset_name = kwargs[\"name\"]\n",
    "\n",
    "    # setting the training parameters\n",
    "    training_args = dpo_config\n",
    "    training_args.output_dir = f\"./{output_folder}/models/{dataset_name}\"\n",
    "    training_args.logging_dir = f\"./{output_folder}/runs/{dataset_name}\"\n",
    "    training_args.max_length = eval(kwargs[\"max_seq_length\"]) + 500\n",
    "    training_args.max_prompt_length = eval(kwargs[\"max_seq_length\"])\n",
    "    training_args.num_train_epochs = num_epochs\n",
    "\n",
    "    learning_rate = kwargs[\"learning_rate\"]\n",
    "    if learning_rate != \"default\":\n",
    "        learning_rate = float(learning_rate)\n",
    "        training_args.learning_rate = learning_rate\n",
    "\n",
    "    # creating the DPO trainer\n",
    "    trainer = DPOTrainer(\n",
    "        model,\n",
    "        args=training_args,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # training the model\n",
    "    trainer.train()\n",
    "    trainer.model.save_pretrained(f\"./{output_folder}/models/{dataset_name}/final\")\n",
    "    tokenizer.save_pretrained(f\"./{output_folder}/models/{dataset_name}/final\")\n",
    "\n",
    "    del train_dataset, test_dataset\n",
    "\n",
    "    for i in range(2):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        time.sleep(30)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def train_SFT(model, tokenizer, num_epochs, output_folder, sft_config, peft_config, **kwargs):\n",
    "    # loading the datasets\n",
    "    train_dataset, test_dataset = load_train_test(**kwargs)\n",
    "\n",
    "    dataset_name = kwargs[\"name\"]\n",
    "    prompt_formatting_function = format_function(dataset_name)\n",
    "\n",
    "    response_template = \" ### Answer:\"\n",
    "    collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "    training_args = sft_config\n",
    "    training_args.output_dir = f\"./{output_folder}/models/{dataset_name}\"\n",
    "    training_args.logging_dir = f\"./{output_folder}/runs/{dataset_name}\"\n",
    "    training_args.max_seq_length = eval(kwargs[\"max_seq_length\"])\n",
    "    training_args.num_train_epochs = num_epochs\n",
    "\n",
    "    learning_rate = kwargs[\"learning_rate\"]\n",
    "    if learning_rate != \"default\":\n",
    "        learning_rate = float(learning_rate)\n",
    "        training_args.learning_rate = learning_rate\n",
    "\n",
    "    # creating the SFT trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        # tokenizer=tokenizer,\n",
    "        formatting_func=prompt_formatting_function,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # training the model\n",
    "    trainer.train()\n",
    "    trainer.model.save_pretrained(f\"./{output_folder}/models/{dataset_name}/final\")\n",
    "    tokenizer.save_pretrained(f\"./{output_folder}/models/{dataset_name}/final\")\n",
    "\n",
    "    del train_dataset, test_dataset\n",
    "\n",
    "    for i in range(2):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        time.sleep(30)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def train(config_list_names: List[str], model_configs: configparser.ConfigParser,\n",
    "          data_configs: configparser.ConfigParser, peft_config: LoraConfig,\n",
    "          sft_config: SFTConfig, dpo_config: DPOConfig) -> int:\n",
    "    \"\"\"\n",
    "    Trains the base model on different configurations and saves them.\n",
    "    For each configuration, the model is trained succesively on the datasets specified in its config file. The training is done in the order of the datasets in the config file.\n",
    "    The model can be trained on SFT or DPO datasets. The type of dataset is specified in the config file, and the specificity of the dataset is specified in the data_info file.\n",
    "\n",
    "    Inputs:\n",
    "        model: a transformer model\n",
    "        config_list_names: a list of the names of the configurations in the config file\n",
    "        model_configs: a configparser object containing the model configurations\n",
    "        data_configs: a configparser object containing the data configurations\n",
    "\n",
    "    Outputs:\n",
    "        0: if the training is successful\n",
    "    \"\"\"\n",
    "\n",
    "    # for each configuration, train the dataset based on the configuration\n",
    "    for config_name in config_list_names:\n",
    "\n",
    "        # loading the base model and tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.bfloat16,\n",
    "                                                     trust_remote_code=True, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True, padding_side='left',\n",
    "                                                  device_map=\"auto\")\n",
    "\n",
    "        # model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True, padding_side='left', device_map=\"auto\")\n",
    "\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # loading training configuration of the configuration name\n",
    "        config = dict(model_configs[config_name].items())\n",
    "        dataset_list = eval(config[\"datasets\"])\n",
    "        epochs_per_dataset = eval(config[\"epochs_per_dataset\"])\n",
    "\n",
    "        # iterating on the configuration datasets\n",
    "        for n, dataset_name in enumerate(dataset_list):\n",
    "\n",
    "            # loading the dataset informations\n",
    "            dataset_config = dict(data_configs[dataset_name].items())\n",
    "            type = dataset_config[\"type\"]\n",
    "\n",
    "            epochs = epochs_per_dataset[n]\n",
    "\n",
    "            # training the model based on the train type\n",
    "            if type == \"DPO\":\n",
    "                train_DPO(model, tokenizer, epochs, config_name, dpo_config, peft_config, **dataset_config)\n",
    "            elif type == \"SFT\":\n",
    "                train_SFT(model, tokenizer, epochs, config_name, sft_config, peft_config, **dataset_config)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid training type\")\n",
    "\n",
    "            # saving the model\n",
    "            # model.save_pretrained(f\"{model_configs[config_name]['save_dir']}/{config_name}_{str(n)}\")\n",
    "            # tokenizer.save_pretrained(f\"{model_configs[config_name]['save_dir']}/{config_name}_{str(n)}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        del (model, tokenizer)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_configs(param_dict: Dict[str, Any]) -> (SFTConfig, DPOConfig, LoraConfig):\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"./output\",\n",
    "\n",
    "        per_device_train_batch_size=int(param_dict[\"per_device_train_batch_size_sft\"]),\n",
    "        per_device_eval_batch_size=int(param_dict[\"per_device_eval_batch_size_sft\"]),\n",
    "        eval_accumulation_steps=int(param_dict[\"eval_accumulation_steps_sft\"]),\n",
    "        gradient_accumulation_steps=int(param_dict[\"gradient_accumulation_steps_sft\"]),\n",
    "\n",
    "        bf16=True,\n",
    "\n",
    "        learning_rate=float(param_dict[\"learning_rate_sft\"]),\n",
    "        lr_scheduler_type=param_dict[\"lr_scheduler_type_sft\"],\n",
    "        warmup_ratio=float(param_dict[\"warmup_ratio_sft\"]),\n",
    "\n",
    "        logging_steps=1,\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "\n",
    "    dpo_config = DPOConfig(\n",
    "        output_dir=\"./output\",\n",
    "\n",
    "        beta=0.1,\n",
    "        per_device_train_batch_size=int(param_dict[\"per_device_train_batch_size_dpo\"]),\n",
    "        per_device_eval_batch_size=int(param_dict[\"per_device_eval_batch_size_dpo\"]),\n",
    "        eval_accumulation_steps=int(param_dict[\"eval_accumulation_steps_dpo\"]),\n",
    "        gradient_accumulation_steps=int(param_dict[\"gradient_accumulation_steps_dpo\"]),\n",
    "\n",
    "        bf16=True,\n",
    "\n",
    "        learning_rate=float(param_dict[\"learning_rate_dpo\"]),\n",
    "        lr_scheduler_type=param_dict[\"lr_scheduler_type_dpo\"],\n",
    "        warmup_ratio=float(param_dict[\"warmup_ratio_dpo\"]),\n",
    "\n",
    "        logging_steps=1,\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"Wqkv\", \"fc1\", \"fc2\"]  # [\"Wqkv\", \"fc1\", \"fc2\" ] # [\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\" ]\n",
    "    )\n",
    "\n",
    "    return peft_config, sft_config, dpo_config"
   ],
   "id": "a6bf14da25930fa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--config_list_names\", type=str, nargs=\"+\", help=\"List of the names of the configurations in the config file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    DATA_FOLDER = \"data\"\n",
    "    config_file = \"config.ini\"\n",
    "    data_info_file = \"dataset_info.ini\"\n",
    "    training_config_file = \"training.ini\"\n",
    "\n",
    "    model_configs = read_config(config_file)\n",
    "    data_configs = read_config(data_info_file)\n",
    "\n",
    "    config_name = \"DEFAULT\"\n",
    "    training_configs = read_config(training_config_file)\n",
    "    peft_config, sft_config, dpo_config = get_configs(dict(training_configs[config_name].items()))\n",
    "\n",
    "    config_list_names = args.config_list_names\n",
    "    # config_list_names = [\"training_31_05_Anton\"]\n",
    "    train(config_list_names, model_configs, data_configs, peft_config, sft_config, dpo_config)\n"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
