{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:37:41.537319Z",
     "start_time": "2024-06-11T21:37:41.533565Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datasets\n",
    "np.random.seed(270)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:37:41.555815Z",
     "start_time": "2024-06-11T21:37:41.539451Z"
    }
   },
   "source": [
    "def read_dfs(data_name, suffixes=[\"train\", \"val\", \"test\"]):\n",
    "    print(f\"loading data: {data_name} ...\")\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(len(suffixes)):\n",
    "        suffix = suffixes[i]\n",
    "        data_sfx = pd.read_json(f\"{data_name}_{suffix}.jsonl\", lines=True)\n",
    "        data = pd.concat([data, data_sfx], axis=0)\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:37:44.581296Z",
     "start_time": "2024-06-11T21:37:41.556815Z"
    }
   },
   "source": [
    "epfl_dpo_data = read_dfs(\"EPFL/raw/EPFL_DPO\", suffixes=[\"train_clipped\", \"test\", \"val\"])\n",
    "epfl_sft_data = read_dfs(\"EPFL/raw/EPFL_SFT\", suffixes=[\"train_clipped\", \"test\", \"val\"])\n",
    "helpsteer_data = read_dfs(\"helpSteer/raw/helpsteer\", suffixes=[\"train_clipped\", \"test\", \"val\"])\n",
    "mathqa_data = read_dfs(\"mathQA/raw/mathQA\", suffixes=[\"train_clipped\", \"val\", \"test\"])\n",
    "mmlu_data = read_dfs(\"MMLU/raw/MMLU\", suffixes=[\"train_clipped\", \"test_clipped\", \"val_clipped\"])\n",
    "openqa_data = read_dfs(\"openQA/raw/openQA\", suffixes=[\"train_clipped\", \"val\", \"test\"])\n",
    "shp_data = read_dfs(\"shp/raw/shp\", suffixes=[\"train_clipped\", \"test_clipped\", \"val_clipped\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: EPFL/raw/EPFL_DPO ...\n",
      "loading data: EPFL/raw/EPFL_SFT ...\n",
      "loading data: helpSteer/raw/helpsteer ...\n",
      "loading data: mathQA/raw/mathQA ...\n",
      "loading data: MMLU/raw/MMLU ...\n",
      "loading data: openQA/raw/openQA ...\n",
      "loading data: shp/raw/shp ...\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:37:45.013941Z",
     "start_time": "2024-06-11T21:37:44.582309Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\",\n",
    "                                              trust_remote_code=True,\n",
    "                                              padding_side='left',\n",
    "                                              add_eos_token=True,\n",
    "                                              add_bos_token=True,\n",
    "                                              use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:37:45.020624Z",
     "start_time": "2024-06-11T21:37:45.014942Z"
    }
   },
   "source": [
    "def formatting_func(example, field):\n",
    "    text = f\"{example[field]}\"\n",
    "    return text\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt, field):\n",
    "    return tokenizer(formatting_func(prompt, field))\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    dataset = pd.read_json(data_path, lines=True)\n",
    "    dataset = dataset.Dataset.from_pandas(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tokenize_dataset(dataset, field):\n",
    "    tokenized_dataset = dataset.map(lambda x: generate_and_tokenize_prompt(x, field))\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def compute_lengths(dataframe, field):\n",
    "    # tokenization and lengths computation\n",
    "    dataset = datasets.Dataset.from_pandas(dataframe)\n",
    "    tokenized_dataset = tokenize_dataset(dataset, field)\n",
    "    lengths = [len(x[field]) for x in tokenized_dataset]\n",
    "\n",
    "    return lengths"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:37:45.040577Z",
     "start_time": "2024-06-11T21:37:45.021693Z"
    }
   },
   "source": [
    "def process_dataframe(dataframe, data_path, fname, field=\"prompt\", max_length=512):\n",
    "    # if data_path directory does not exist, create it\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    print(f\"processing dataframe {fname}...\")\n",
    "\n",
    "    # shuffling the dataframe\n",
    "    dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "    print(f\"Length before clipping: {len(dataframe)}\")\n",
    "\n",
    "    # lengths computation \n",
    "    lengths = compute_lengths(dataframe, field)\n",
    "\n",
    "    # filtering out the examples that exceed the max_length\n",
    "    dataframe[\"lengths\"] = lengths\n",
    "    dataframe = dataframe[dataframe[\"lengths\"] <= max_length]\n",
    "    dataframe = dataframe.drop(columns=[\"lengths\"])\n",
    "    print(f\"Length after clipping: {len(dataframe)}\\n\")\n",
    "\n",
    "    #splitting 60-train / 20-test / 20-val\n",
    "    train_size = int(0.6 * len(dataframe))\n",
    "    val_size = int(0.2 * len(dataframe))\n",
    "\n",
    "    train_df = dataframe[:train_size]\n",
    "    val_df = dataframe[train_size:train_size+val_size]\n",
    "    test_df = dataframe[train_size+val_size:]\n",
    "\n",
    "    # saving the dataframes\n",
    "    train_df.to_json(f\"{data_path}/{fname}_train.jsonl\", orient=\"records\", lines=True)\n",
    "    val_df.to_json(f\"{data_path}/{fname}_val.jsonl\", orient=\"records\", lines=True)\n",
    "    test_df.to_json(f\"{data_path}/{fname}_test.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    return 0"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:43:25.167832Z",
     "start_time": "2024-06-11T21:37:45.043593Z"
    }
   },
   "source": [
    "# shuffling the data\n",
    "np.random.seed(270)\n",
    "\n",
    "process_dataframe(epfl_dpo_data, \"EPFL/processed\", \"EPFL_DPO\", field=\"prompt\")\n",
    "process_dataframe(epfl_sft_data, \"EPFL/processed\", \"EPFL_SFT\", field=\"prompt\")\n",
    "process_dataframe(helpsteer_data, \"helpSteer/processed\", \"helpsteer\", field=\"prompt\")\n",
    "process_dataframe(mathqa_data, \"mathQA/processed\", \"mathQA\", field=\"question\")\n",
    "process_dataframe(mmlu_data, \"MMLU/processed\", \"MMLU\", field=\"question\")\n",
    "process_dataframe(openqa_data, \"openQA/processed\", \"openQA\", field=\"question\")\n",
    "process_dataframe(shp_data, \"shp/processed\", \"shp\", field=\"prompt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataframe EPFL_DPO...\n",
      "Length before clipping: 12614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12614 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a359457996c54d5486b558eee83c2b08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 9188\n",
      "\n",
      "processing dataframe EPFL_SFT...\n",
      "Length before clipping: 8378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/8378 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "041963d7ec7c420aa3e33d9dd3529d7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 6149\n",
      "\n",
      "processing dataframe helpsteer...\n",
      "Length before clipping: 15860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15860 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24cdbc2ed9a9466e8847ebd0af2a55da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 10680\n",
      "\n",
      "processing dataframe mathQA...\n",
      "Length before clipping: 33426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/33426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2003af0e1ed420482ab4d5eb53f0ce0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 33121\n",
      "\n",
      "processing dataframe MMLU...\n",
      "Length before clipping: 105488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/105488 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7e08c8e99b14671bfb68c4bdc0b13d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 22357\n",
      "\n",
      "processing dataframe openQA...\n",
      "Length before clipping: 5457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5457 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9322b5cebbe4f8b9ce49a75e2c8fc46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 5456\n",
      "\n",
      "processing dataframe shp...\n",
      "Length before clipping: 122096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/122096 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2baf490307a451cad9e7e15ca410139"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 64676\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:50:23.169283Z",
     "start_time": "2024-06-11T21:50:22.748928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shp_test = pd.read_json(\"shp/processed/shp_test.jsonl\", lines=True)\n",
    "shp_train = pd.read_json(\"shp/processed/shp_train.jsonl\", lines=True)\n",
    "shp_val = pd.read_json(\"shp/processed/shp_val.jsonl\", lines=True)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:52:36.781488Z",
     "start_time": "2024-06-11T21:52:36.351802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shp_sft_train = shp_train.iloc[:len(shp_train)//2]\n",
    "shp_sft_val = shp_val.iloc[:len(shp_val)//2]\n",
    "shp_sft_test = shp_test.iloc[:len(shp_test)//2]\n",
    "\n",
    "shp_dpo_train = shp_train.iloc[len(shp_train)//2:]\n",
    "shp_dpo_val = shp_val.iloc[len(shp_val)//2:]\n",
    "shp_dpo_test = shp_test.iloc[len(shp_test)//2:]\n",
    "\n",
    "shp_sft_train.to_json(\"shp/processed/shp_SFT_train.jsonl\", orient=\"records\", lines=True)\n",
    "shp_sft_val.to_json(\"shp/processed/shp_SFT_val.jsonl\", orient=\"records\", lines=True)\n",
    "shp_sft_test.to_json(\"shp/processed/shp_SFT_test.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "shp_dpo_train.to_json(\"shp/processed/shp_DPO_train.jsonl\", orient=\"records\", lines=True)\n",
    "shp_dpo_val.to_json(\"shp/processed/shp_DPO_val.jsonl\", orient=\"records\", lines=True)\n",
    "shp_dpo_test.to_json(\"shp/processed/shp_DPO_test.jsonl\", orient=\"records\", lines=True)"
   ],
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3-mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
