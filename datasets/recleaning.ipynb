{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:10:40.233897Z",
     "start_time": "2024-06-12T20:10:40.230377Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datasets\n",
    "np.random.seed(270)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:10:40.244896Z",
     "start_time": "2024-06-12T20:10:40.237901Z"
    }
   },
   "source": [
    "def read_dfs(data_name, suffixes=[\"train\", \"val\", \"test\"]):\n",
    "    print(f\"loading data: {data_name} ...\")\n",
    "    data = pd.DataFrame()\n",
    "    if suffixes is None:\n",
    "        data = pd.read_json(f\"{data_name}.jsonl\", lines=True)\n",
    "        return data\n",
    "    \n",
    "    for i in range(len(suffixes)):\n",
    "        suffix = suffixes[i]\n",
    "        data_sfx = pd.read_json(f\"{data_name}_{suffix}.jsonl\", lines=True)\n",
    "        data = pd.concat([data, data_sfx], axis=0)\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:10:43.648741Z",
     "start_time": "2024-06-12T20:10:40.246891Z"
    }
   },
   "source": [
    "epfl_dpo_data = read_dfs(\"EPFL/raw/EPFL_DPO\", suffixes=[\"train_clipped\", \"test\", \"val\"])\n",
    "epfl_sft_data = read_dfs(\"EPFL/raw/EPFL_SFT\", suffixes=[\"train_clipped\", \"test\", \"val\"])\n",
    "helpsteer_data = read_dfs(\"helpSteer/raw/helpsteer\", suffixes=[\"train_clipped\", \"test\", \"val\"])\n",
    "mathqa_data = read_dfs(\"mathQA/raw/mathQA\", suffixes=[\"train_clipped\", \"val\", \"test\"])\n",
    "mmlu_data = read_dfs(\"MMLU/raw/MMLU\", suffixes=[\"train_clipped\", \"test_clipped\", \"val_clipped\"])\n",
    "openqa_data = read_dfs(\"openQA/raw/openQA\", suffixes=[\"train_clipped\", \"val\", \"test\"])\n",
    "shp_data = read_dfs(\"shp/raw/shp\", suffixes=[\"train_clipped\", \"test_clipped\", \"val_clipped\"])\n",
    "\n",
    "scienceQA_data = read_dfs(\"scienceQA/raw/scienceQA_all\", suffixes=None)\n",
    "tal_data = read_dfs(\"tal_scq5k/raw/tal_scq5k_train\", suffixes=None)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: EPFL/raw/EPFL_DPO ...\n",
      "loading data: EPFL/raw/EPFL_SFT ...\n",
      "loading data: helpSteer/raw/helpsteer ...\n",
      "loading data: mathQA/raw/mathQA ...\n",
      "loading data: MMLU/raw/MMLU ...\n",
      "loading data: openQA/raw/openQA ...\n",
      "loading data: shp/raw/shp ...\n",
      "loading data: scienceQA/raw/scienceQA_all ...\n",
      "loading data: tal_scq5k/raw/tal_scq5k_train ...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:10:45.692947Z",
     "start_time": "2024-06-12T20:10:43.649745Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\",\n",
    "                                              trust_remote_code=True,\n",
    "                                              padding_side='left',\n",
    "                                              add_eos_token=True,\n",
    "                                              add_bos_token=True,\n",
    "                                              use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:10:45.701471Z",
     "start_time": "2024-06-12T20:10:45.694472Z"
    }
   },
   "source": [
    "def formatting_func(example, field):\n",
    "    text = f\"{example[field]}\"\n",
    "    return text\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt, field):\n",
    "    return tokenizer(formatting_func(prompt, field))\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    dataset = pd.read_json(data_path, lines=True)\n",
    "    dataset = dataset.Dataset.from_pandas(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tokenize_dataset(dataset, field):\n",
    "    tokenized_dataset = dataset.map(lambda x: generate_and_tokenize_prompt(x, field))\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def compute_lengths(dataframe, field):\n",
    "    # tokenization and lengths computation\n",
    "    dataset = datasets.Dataset.from_pandas(dataframe)\n",
    "    tokenized_dataset = tokenize_dataset(dataset, field)\n",
    "    lengths = [len(x[field]) for x in tokenized_dataset]\n",
    "\n",
    "    return lengths"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:10:45.709475Z",
     "start_time": "2024-06-12T20:10:45.702473Z"
    }
   },
   "source": [
    "def process_dataframe(dataframe, data_path, fname, field=\"prompt\", max_length=512):\n",
    "    # if data_path directory does not exist, create it\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    print(f\"processing dataframe {fname}...\")\n",
    "\n",
    "    # shuffling the dataframe\n",
    "    dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "    print(f\"Length before clipping: {len(dataframe)}\")\n",
    "\n",
    "    # lengths computation \n",
    "    lengths = compute_lengths(dataframe, field)\n",
    "\n",
    "    # filtering out the examples that exceed the max_length\n",
    "    dataframe[\"lengths\"] = lengths\n",
    "    dataframe = dataframe[dataframe[\"lengths\"] <= max_length]\n",
    "    dataframe = dataframe.drop(columns=[\"lengths\"])\n",
    "    print(f\"Length after clipping: {len(dataframe)}\\n\")\n",
    "\n",
    "    #splitting 50-train / 25-test / 15-test quantization / 15-val\n",
    "    train_size = int(0.5 * len(dataframe))\n",
    "    test_size = int(0.25 * len(dataframe))\n",
    "    test_quant_size = int(0.15 * len(dataframe))\n",
    "\n",
    "    train_df = dataframe[:train_size]\n",
    "    test_df = dataframe[train_size:train_size+test_size]\n",
    "    test_quant_df = dataframe[train_size+test_size:train_size+test_size+test_quant_size]\n",
    "    val_df = dataframe[train_size+test_size+test_quant_size:]\n",
    "    \n",
    "    # saving the dataframes\n",
    "    train_df.to_json(f\"{data_path}/{fname}_train.jsonl\", orient=\"records\", lines=True)\n",
    "    test_df.to_json(f\"{data_path}/{fname}_test.jsonl\", orient=\"records\", lines=True)\n",
    "    test_quant_df.to_json(f\"{data_path}/{fname}_test_quantization.jsonl\", orient=\"records\", lines=True)\n",
    "    val_df.to_json(f\"{data_path}/{fname}_val.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    return 0"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:17:33.647928Z",
     "start_time": "2024-06-12T20:17:15.890184Z"
    }
   },
   "source": [
    "# shuffling the data\n",
    "np.random.seed(270)\n",
    "\n",
    "process_dataframe(epfl_dpo_data, \"EPFL/processed\", \"EPFL_DPO\", field=\"prompt\")\n",
    "process_dataframe(epfl_sft_data, \"EPFL/processed\", \"EPFL_SFT\", field=\"prompt\")\n",
    "process_dataframe(helpsteer_data, \"helpSteer/processed\", \"helpsteer\", field=\"prompt\")\n",
    "process_dataframe(mathqa_data, \"mathQA/processed\", \"mathQA\", field=\"question\")\n",
    "process_dataframe(mmlu_data, \"MMLU/processed\", \"MMLU\", field=\"question\")\n",
    "process_dataframe(openqa_data, \"openQA/processed\", \"openQA\", field=\"question\")\n",
    "process_dataframe(shp_data, \"shp/processed\", \"shp\", field=\"prompt\")\n",
    "\n",
    "process_dataframe(scienceQA_data, \"scienceQA/processed\", \"scienceQA_all\", field=\"question\")\n",
    "process_dataframe(tal_data, \"tal_scq5k/processed\", \"tal_scq5k_train\", field=\"question\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataframe scienceQA_all...\n",
      "Length before clipping: 41226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/41226 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cd9197f43f549c6b2adf1901f299ac8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 41023\n",
      "\n",
      "processing dataframe tal_scq5k_train...\n",
      "Length before clipping: 3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d0df87c91994b65a451b8b1cc0d2db4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after clipping: 2912\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3-mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
